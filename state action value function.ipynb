{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7bb50b3-5a34-4559-8897-866d75db708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f67eeb9-5583-42a3-b457-c85ae6b38301",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 6 \n",
    "num_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a71eb7e6-b6bc-418f-bf70-8982cf1717b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_left_reward = 100\n",
    "terminal_right_reward = 40\n",
    "each_step_reward = 0\n",
    "\n",
    "gamma = 0.5 #discount factor\n",
    "misstep_prob = 0  #probability of going in the wrong direction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "784c6181-d022-4e42-b721-afa5a2b067fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def generate_rewards(num_states, each_step_reward, terminal_left_reward, terminal_right_reward):\n",
      "\n",
      "    rewards = [each_step_reward] * num_states\n",
      "    rewards[0] = terminal_left_reward\n",
      "    rewards[-1] = terminal_right_reward\n",
      "    \n",
      "    return rewards \n",
      "\n",
      "def generate_transition_prob(num_states, num_actions, misstep_prob = 0):\n",
      "    # 0 is left, 1 is right \n",
      "    \n",
      "    p = np.zeros((num_states, num_actions, num_states))\n",
      "    \n",
      "    for i in range(num_states):        \n",
      "        if i != 0:\n",
      "            p[i, 0, i-1] = 1 - misstep_prob\n",
      "            p[i, 1, i-1] = misstep_prob\n",
      "            \n",
      "        if i != num_states - 1:\n",
      "            p[i, 1, i+1] = 1  - misstep_prob\n",
      "            p[i, 0, i+1] = misstep_prob\n",
      "        \n",
      "    # Terminal States    \n",
      "    p[0] = np.zeros((num_actions, num_states))\n",
      "    p[-1] = np.zeros((num_actions, num_states))\n",
      "    \n",
      "    return p\n",
      "\n",
      "def calculate_Q_value(num_states, rewards, transition_prob, gamma, V_states, state, action):\n",
      "    q_sa = rewards[state] + gamma * sum([transition_prob[state, action, sp] * V_states[sp] for sp in range(num_states)])\n",
      "    return q_sa\n",
      "\n",
      "def evaluate_policy(num_states, rewards, transition_prob, gamma, policy):\n",
      "    max_policy_eval = 10000 \n",
      "    threshold = 1e-10\n",
      "    \n",
      "    V = np.zeros(num_states)\n",
      "    \n",
      "    for i in range(max_policy_eval):\n",
      "        delta = 0\n",
      "        for s in range(num_states):\n",
      "            v = V[s]\n",
      "            V[s] = calculate_Q_value(num_states, rewards, transition_prob, gamma, V, s, policy[s])\n",
      "            delta = max(delta, abs(v - V[s]))\n",
      "                       \n",
      "        if delta < threshold:\n",
      "            break\n",
      "            \n",
      "    return V\n",
      "\n",
      "def improve_policy(num_states, num_actions, rewards, transition_prob, gamma, V, policy):\n",
      "    policy_stable = True\n",
      "    \n",
      "    for s in range(num_states):\n",
      "        q_best = V[s]\n",
      "        for a in range(num_actions):\n",
      "            q_sa = calculate_Q_value(num_states, rewards, transition_prob, gamma, V, s, a)\n",
      "            if q_sa > q_best and policy[s] != a:\n",
      "                policy[s] = a\n",
      "                q_best = q_sa\n",
      "                policy_stable = False\n",
      "    \n",
      "    return policy, policy_stable\n",
      "\n",
      "\n",
      "def get_optimal_policy(num_states, num_actions, rewards, transition_prob, gamma):\n",
      "    optimal_policy = np.zeros(num_states, dtype=int)\n",
      "    max_policy_iter = 10000 \n",
      "\n",
      "    for i in range(max_policy_iter):\n",
      "        policy_stable = True\n",
      "\n",
      "        V = evaluate_policy(num_states, rewards, transition_prob, gamma, optimal_policy)\n",
      "        optimal_policy, policy_stable = improve_policy(num_states, num_actions, rewards, transition_prob, gamma, V, optimal_policy)\n",
      "\n",
      "        if policy_stable:\n",
      "            break\n",
      "            \n",
      "    return optimal_policy, V\n",
      "\n",
      "def calculate_Q_values(num_states, rewards, transition_prob, gamma, optimal_policy):\n",
      "    # Left and then optimal policy\n",
      "    q_left_star = np.zeros(num_states)\n",
      "\n",
      "    # Right and optimal policy\n",
      "    q_right_star = np.zeros(num_states)\n",
      "    \n",
      "    V_star =  evaluate_policy(num_states, rewards, transition_prob, gamma, optimal_policy)\n",
      "\n",
      "    for s in range(num_states):\n",
      "        q_left_star[s] = calculate_Q_value(num_states, rewards, transition_prob, gamma, V_star, s, 0)\n",
      "        q_right_star[s] = calculate_Q_value(num_states, rewards, transition_prob, gamma, V_star, s, 1)\n",
      "        \n",
      "    return q_left_star, q_right_star\n",
      "\n",
      "\n",
      "def plot_optimal_policy_return(num_states, optimal_policy, rewards, V):\n",
      "    actions = [r\"$\\leftarrow$\" if a == 0 else r\"$\\rightarrow$\" for a in optimal_policy]\n",
      "    actions[0] = \"\"\n",
      "    actions[-1] = \"\"\n",
      "    \n",
      "    fig, ax = plt.subplots(figsize=(2*num_states,2))\n",
      "\n",
      "    for i in range(num_states):\n",
      "        ax.text(i+0.5, 0.5, actions[i], fontsize=32, ha=\"center\", va=\"center\", color=\"orange\")\n",
      "        ax.text(i+0.5, 0.25, rewards[i], fontsize=16, ha=\"center\", va=\"center\", color=\"black\")\n",
      "        ax.text(i+0.5, 0.75, round(V[i],2), fontsize=16, ha=\"center\", va=\"center\", color=\"firebrick\")\n",
      "        ax.axvline(i, color=\"black\")\n",
      "    ax.set_xlim([0, num_states])\n",
      "    ax.set_ylim([0, 1])\n",
      "\n",
      "    ax.set_xticklabels([])\n",
      "    ax.set_yticklabels([])\n",
      "    ax.tick_params(axis='both', which='both', length=0)\n",
      "    ax.set_title(\"Optimal policy\",fontsize = 16)\n",
      "\n",
      "def plot_q_values(num_states, q_left_star, q_right_star, rewards):\n",
      "    fig, ax = plt.subplots(figsize=(3*num_states,2))\n",
      "\n",
      "    for i in range(num_states):\n",
      "        ax.text(i+0.2, 0.6, round(q_left_star[i],2), fontsize=16, ha=\"center\", va=\"center\", color=\"firebrick\")\n",
      "        ax.text(i+0.8, 0.6, round(q_right_star[i],2), fontsize=16, ha=\"center\", va=\"center\", color=\"firebrick\")\n",
      "\n",
      "        ax.text(i+0.5, 0.25, rewards[i], fontsize=20, ha=\"center\", va=\"center\", color=\"black\")\n",
      "        ax.axvline(i, color=\"black\")\n",
      "    ax.set_xlim([0, num_states])\n",
      "    ax.set_ylim([0, 1])\n",
      "\n",
      "    ax.set_xticklabels([])\n",
      "    ax.set_yticklabels([])\n",
      "    ax.tick_params(axis='both', which='both', length=0)\n",
      "    ax.set_title(\"Q(s,a)\",fontsize = 16)\n",
      "\n",
      "def generate_visualization(terminal_left_reward, terminal_right_reward, each_step_reward, gamma, misstep_prob):\n",
      "    num_states = 6\n",
      "    num_actions = 2\n",
      "    \n",
      "    rewards = generate_rewards(num_states, each_step_reward, terminal_left_reward, terminal_right_reward)\n",
      "    transition_prob = generate_transition_prob(num_states, num_actions, misstep_prob)\n",
      "    \n",
      "    optimal_policy, V = get_optimal_policy(num_states, num_actions, rewards, transition_prob, gamma)\n",
      "    q_left_star, q_right_star = calculate_Q_values(num_states, rewards, transition_prob, gamma, optimal_policy)\n",
      "    \n",
      "    plot_optimal_policy_return(num_states, optimal_policy, rewards, V)\n",
      "    plot_q_values(num_states, q_left_star, q_right_star, rewards)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# الطريقة 1: نفتح الملف ونقرأه\n",
    "with open(\"utils.py\", \"r\") as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7706e0f-b7ef-4fdd-8d88-d85219e472e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_visualization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generate_visualization(terminal_left_reward, terminal_right_reward, each_step_reward, gamma, misstep_prob)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_visualization' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "generate_visualization(terminal_left_reward, terminal_right_reward, each_step_reward, gamma, misstep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f794b78-0daf-4664-ac5e-249933dc1ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9590c2b-f9c3-4828-a24b-b82fea8acc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0ac33-7d10-4a68-946e-455055ccb544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
